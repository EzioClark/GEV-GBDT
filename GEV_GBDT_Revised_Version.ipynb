{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bKiE8Pk9zih"
      },
      "outputs": [],
      "source": [
        "!pip install hmeasure"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import KFold, TimeSeriesSplit\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# Generate synthetic data with unbalanced classes\n",
        "n_samples = 200\n",
        "X = np.ones(n_samples)\n",
        "y = np.hstack([np.zeros(int(n_samples * 0.3)), np.ones(int(n_samples * 0.7))])\n",
        "\n",
        "# Shuffle the data to randomly distribute the class labels\n",
        "X, y = shuffle(X, y, random_state=42)\n",
        "\n",
        "# K-Fold Cross-Validation\n",
        "kf = KFold(n_splits=5)\n",
        "kf_splits = list(kf.split(X, y))\n",
        "\n",
        "# Time Series Cross-Validation\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "tscv_splits = list(tscv.split(X))\n",
        "\n",
        "# Plotting function\n",
        "def plot_cv_indices(cv, X, y, ax, n_splits, title, lw=10):\n",
        "    cmap_data = plt.cm.Paired\n",
        "    for ii, (train, test) in enumerate(cv):\n",
        "        indices = np.arange(len(X))\n",
        "        ax.scatter(indices[train], [ii + 0.5] * len(train), c=cmap_data(0), marker='_', lw=lw)\n",
        "        ax.scatter(indices[test], [ii + 0.5] * len(test), c=cmap_data(1), marker='_', lw=lw)\n",
        "    ax.scatter(indices, [ii + 1.5] * len(X), c=cmap_data(y), marker='|', lw=lw)\n",
        "    ax.set(ylim=[n_splits + 1.5, -0.2], xlim=[0, len(X)], xlabel='Sample index', ylabel='CV iteration',\n",
        "           yticks=np.arange(n_splits + 1) + 0.5, yticklabels=list(range(n_splits)) + ['class'])\n",
        "    ax.legend(['Training set', 'Testing set'], loc='upper right')\n",
        "    ax.set_title(title, fontsize=15)\n",
        "\n",
        "# Plotting the cross-validation splits\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "plot_cv_indices(kf_splits, X, y, axes[0], len(kf_splits), 'K-Fold Cross-Validation (k=5)')\n",
        "plot_cv_indices(tscv_splits, X, y, axes[1], len(tscv_splits), 'Time Series Cross-Validation (5 Splits)')\n",
        "\n",
        "# Adding overall plot title and captions\n",
        "fig.suptitle('Comparison of K-Fold and Time Series Cross-Validation', fontsize=20)\n",
        "axes[0].set_ylabel('CV Iteration and Class Labels', fontsize=12)\n",
        "axes[0].set_xlabel('Sample Index', fontsize=12)\n",
        "axes[1].set_ylabel('CV Iteration and Class Labels', fontsize=12)\n",
        "axes[1].set_xlabel('Sample Index', fontsize=12)\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "plt.savefig('CV_Compare.png', dpi=300)\n"
      ],
      "metadata": {
        "id": "6LLhLqCQdPcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YeKwoETQ5wcT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "# A Lite/fast/popular gradient boosting tree\n",
        "import lightgbm as lgb\n",
        "from sklearn import preprocessing\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import metrics\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "from hmeasure import h_score\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_auc_score, confusion_matrix, roc_curve, auc\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.calibration import calibration_curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKZXT7IxfLDc"
      },
      "outputs": [],
      "source": [
        "def plot_calibration_curve(y_true, y_prob, ax, label, n_bins=100):\n",
        "    \"\"\"\n",
        "    Plot the calibration curve for a classifier on given axes.\n",
        "\n",
        "    Args:\n",
        "    y_true: array-like of shape (n_samples,), True binary labels.\n",
        "    y_prob: array-like of shape (n_samples,), Target scores, can either be probability estimates of the positive class.\n",
        "    ax: matplotlib.axes.Axes, the axes object to plot on.\n",
        "    label: str, label for the plot curve.\n",
        "    n_bins: int, Number of probability bins to use.\n",
        "    \"\"\"\n",
        "    # Calculate the calibration curve\n",
        "    prob_true, prob_pred = calibration_curve(y_true, y_prob, n_bins=n_bins, strategy='uniform')\n",
        "\n",
        "    # Plot the calibration curve on the given axes\n",
        "    ax.plot(prob_pred, prob_true, marker='o', linewidth=1, label=label)\n",
        "\n",
        "    # Plot the perfectly calibrated line if it's the first fold\n",
        "    if label == 'Fold 1':\n",
        "        ax.plot([0, 1], [0, 1], linestyle='--', color='red', label='Perfectly calibrated')\n",
        "\n",
        "    ax.set_xlabel('Predicted probability')\n",
        "    ax.set_ylabel('Actual probability')\n",
        "    ax.set_title('Calibration Plot')\n",
        "    ax.legend()\n",
        "    ax.grid(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up the GEV-GBDT"
      ],
      "metadata": {
        "id": "4_JmndCSdPA8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfRRs6yX57az"
      },
      "outputs": [],
      "source": [
        "# Define customized loss function\n",
        "\n",
        "def GEV_Loss(y_pred, y_true, tau):\n",
        "    y = y_true.get_label()\n",
        "    x = y_pred\n",
        "    t = tau\n",
        "    grad = (((1-y)*np.exp(-(t*x+1)**(-1/t)))*((t*x+1)**(-1/t-1)))/(1-np.exp(-(t*x+1)**(-1/t))) - y*((t*x+1)**(-1/t-1))\n",
        "    hess = (1/t+1)*t*y*((t*x+1)**(-1/t-2)) + (1-y)*( (np.exp(-(t*x+1)**(-1/t))*((t*x+1)**(-2/t-2)))/(1-np.exp(-(t*x+1)**(-1/t)))+\n",
        "                                                  (np.exp(-2*((t*x+1)**(-1/t)))*((t*x+1)**(-2/t-2)))/((1-np.exp(-(t*x+1)**(-1/t)))**2)-\n",
        "                                                ((1/t+1)*t*np.exp(-(t*x+1)**(-1/t))*((t*x+1)**(-1/t-2)))/(1-np.exp(-(t*x+1)**(-1/t))) )\n",
        "    # grad : first-order partial derivative of y_pred\n",
        "    # hess : second - order partial derivative of y_pred\n",
        "    return grad, hess\n",
        "\n",
        "def Gumbel(y_pred, y_true):\n",
        "    # When tau is set to 0, the GEV function follows gumbel distribution\n",
        "    y = y_true.get_label()\n",
        "    x = y_pred\n",
        "    grad = (np.exp(x)*(np.exp(np.exp(x))*y-1))/(np.exp(np.exp(x))-1)\n",
        "    hess = np.exp(x)*y - ((np.exp(x-np.exp(x))*(1-np.exp(x)))/(1-np.exp(-np.exp(x)))-\n",
        "                          (np.exp(2*x-2*np.exp(x)))/((1-np.exp(-np.exp(x)))**2))*(1-y)\n",
        "\n",
        "    return grad, hess\n",
        "\n",
        "def custom_loss(y_pred, y_true):\n",
        "\n",
        "    if tau == 0:\n",
        "        return Gumbel(y_pred = y_pred, y_true = y_true)\n",
        "    else:\n",
        "        return GEV_Loss(y_pred = y_pred, y_true = y_true, tau = tau)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3eMRnMAB9bNw"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import genextreme as gev\n",
        "\n",
        "def transform_gev(data, tau):\n",
        "    \"\"\"\n",
        "    Transform a 1D array with the Generalized Extreme Value (GEV) distribution.\n",
        "\n",
        "    Parameters:\n",
        "    - data: ndarray, the 1D array of data points to transform.\n",
        "    - tau: float, the shape parameter of the GEV distribution.\n",
        "\n",
        "    Returns:\n",
        "    - transformed_data: ndarray, the transformed data using the GEV CDF.\n",
        "    \"\"\"\n",
        "    # Estimate location and scale parameters from the data\n",
        "    # Here, we use the provided tau (shape parameter) directly\n",
        "    c, loc, scale = tau, np.mean(data), np.std(data)\n",
        "\n",
        "    # Transform data using the GEV CDF with estimated parameters\n",
        "    transformed_data = gev.cdf(data, c, loc=loc, scale=scale)\n",
        "\n",
        "    return transformed_data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### An Example of Using the GEV-GBDT"
      ],
      "metadata": {
        "id": "8orzjM91fFMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the value of tau\n",
        "tau = -0.35\n",
        "\n",
        "# Set the hyperparameter of LightGBM\n",
        "# Remember to set objective as our customised loss function\n",
        "params = {\"objective\":\"binary\", \"boosting_type\":\"gbdt\", \"verbose\": -1, \"metric\": \"auc\", 'learning_rate': 0.01, 'max_depth': -1,  \"objective\": custom_loss}\n",
        "\n",
        "# Assume we have X_train, y_train, X_test, y_test\n",
        "lgb_train = lgb.Dataset(X_train, y_train)\n",
        "lgb_test = lgb.Dataset(X_test, y_test, reference = lgb_train)\n",
        "\n",
        "# Fit the training data\n",
        "gbm = lgb.train(params,\n",
        "                lgb_train,\n",
        "                num_boost_round = 1000)\n",
        "\n",
        "# Predict on the test set and output probability\n",
        "probs = gbm.predict(X_test)\n",
        "# Use link function to transform the raw output within 0 and 1\n",
        "probs = transform_gev(probs, tau)"
      ],
      "metadata": {
        "id": "XjyIZtNAfKHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTJBpE8l3Pfs"
      },
      "source": [
        "## LC Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DpikJMjT8AaG"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, X, y, loss_col_name, fig_name):\n",
        "    tscv = TimeSeriesSplit(n_splits=5)\n",
        "    fig, ax = plt.subplots(figsize=(12,8))\n",
        "    auc_scores = []\n",
        "    ks_stats = []\n",
        "    h_measures = []\n",
        "    cm_sum = np.array([[0, 0], [0, 0]]) # Initialize a confusion matrix sum\n",
        "    actual_losses = []\n",
        "    predicted_losses = []\n",
        "    predicted_losses_in = []\n",
        "    fold = 1\n",
        "    for train_index, test_index in tscv.split(X):\n",
        "        print('Fold trial starts...')\n",
        "        X_train, X_test = X[train_index], X[test_index]\n",
        "        y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "        model.fit(X_train, y_train)\n",
        "        probs = model.predict_proba(X_test)[:, 1]\n",
        "        print('Calculating metrics...')\n",
        "        # Calculate metrics\n",
        "        print('Calculating auc...')\n",
        "        auc_score = roc_auc_score(y_test, probs)\n",
        "        print('Calculating fpr tpr...')\n",
        "        fpr, tpr, thresholds = roc_curve(y_test, probs)\n",
        "        ks_stat = max(abs(fpr - tpr))\n",
        "        print('Calculating hscore...')\n",
        "        h_measure_score = h_score(y_test, probs) # Assuming h_score is given\n",
        "        print('Metrics calculating done...')\n",
        "\n",
        "        # Append scores\n",
        "        auc_scores.append(auc_score)\n",
        "        ks_stats.append(ks_stat)\n",
        "        h_measures.append(h_measure_score)\n",
        "        print('Appending result done...')\n",
        "        # Find best cutoff, generate confusion matrix, and calculate losses\n",
        "        best_cutoff = thresholds[np.argmax(tpr - fpr)]\n",
        "        y_pred = (probs >= best_cutoff).astype(int)\n",
        "        cm = confusion_matrix(y_test, y_pred)\n",
        "        cm_sum += cm\n",
        "        print('Confusion matrix done...')\n",
        "        actual_loss = np.sum(X_test[:, loss_col_name] * y_test)\n",
        "        predicted_loss = np.sum(X_test[:, loss_col_name] * y_pred)\n",
        "        predicted_loss_in = np.sum(X_test[:, loss_col_name] * y_pred * y_test)\n",
        "        actual_losses.append(actual_loss)\n",
        "        predicted_losses.append(predicted_loss)\n",
        "        predicted_losses_in.append(predicted_loss_in)\n",
        "\n",
        "        print('Plotting...')\n",
        "        # Plot Predictive vs Real (for each fold)\n",
        "        plot_calibration_curve(y_test, probs, ax, label=f'Fold {fold}')\n",
        "        fold += 1\n",
        "    plt.savefig(fig_name + '.png', dpi=300)\n",
        "    # Compute averages of scores and losses\n",
        "    avg_auc = np.mean(auc_scores)\n",
        "    avg_ks = np.mean(ks_stats)\n",
        "    avg_h_measure = np.mean(h_measures)\n",
        "    avg_actual_loss = np.mean(actual_losses)\n",
        "    avg_predicted_loss = np.mean(predicted_losses)\n",
        "    avg_predicted_loss_in = np.mean(predicted_losses_in)\n",
        "    loss_difference = avg_actual_loss - avg_predicted_loss\n",
        "    loss_difference_in = avg_actual_loss - avg_predicted_loss_in\n",
        "\n",
        "    # Print average metrics and loss information\n",
        "    print(f\"Average AUC Score: {avg_auc}\")\n",
        "    print(f\"Average KS Statistic: {avg_ks}\")\n",
        "    print(f\"Average H-Measure: {avg_h_measure}\")\n",
        "    print(f\"Cumulative Confusion Matrix:\\n{cm_sum}\")\n",
        "    print(f\"Average Actual Loss: {avg_actual_loss}\")\n",
        "    print(f\"Average Predicted Loss: {avg_predicted_loss}\")\n",
        "    print(f\"Average Predicted Loss: {avg_predicted_loss_in}\")\n",
        "    print(f\"Loss Difference: {loss_difference}\")\n",
        "    print(f\"Loss Difference: {loss_difference_in}\")\n",
        "\n",
        "    # Return metrics and losses for further use if needed\n",
        "    return avg_auc, avg_ks, avg_h_measure, cm_sum, avg_actual_loss, avg_predicted_loss, avg_predicted_loss_in, loss_difference, loss_difference_in\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btWmOg8y4GbH"
      },
      "outputs": [],
      "source": [
        "path = '/content/drive/MyDrive/PhD File Sync/GEVGBDT Things/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rm07tatfwgQO"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CuNXQs9kzHIr"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUMAoL9btQ9x"
      },
      "outputs": [],
      "source": [
        "def evaluate_model_smote(model, X, y, loss_col_name, fig_name):\n",
        "    tscv = TimeSeriesSplit(n_splits=5)\n",
        "    fig, ax = plt.subplots(figsize=(12,8))\n",
        "    auc_scores = []\n",
        "    ks_stats = []\n",
        "    h_measures = []\n",
        "    cm_sum = np.array([[0, 0], [0, 0]]) # Initialize a confusion matrix sum\n",
        "    actual_losses = []\n",
        "    predicted_losses = []\n",
        "    predicted_losses_in = []\n",
        "    fold = 1\n",
        "    for train_index, test_index in tscv.split(X):\n",
        "        print('Fold trial starts...')\n",
        "\n",
        "        X_train, X_test = X[train_index], X[test_index]\n",
        "        y_train, y_test = y[train_index], y[test_index]\n",
        "        print(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train==1)))\n",
        "        print(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train==0)))\n",
        "        sm = SMOTE(random_state = 202402)\n",
        "        X_tr_res, y_tr_res = sm.fit_resample(X_train, y_train)\n",
        "\n",
        "        print('After OverSampling, the shape of train_X: {}'.format(X_tr_res.shape))\n",
        "        print('After OverSampling, the shape of train_y: {} \\n'.format(y_tr_res.shape))\n",
        "\n",
        "        print(\"After OverSampling, counts of label '1': {}\".format(sum(y_tr_res==1)))\n",
        "        print(\"After OverSampling, counts of label '0': {}\".format(sum(y_tr_res==0)))\n",
        "\n",
        "        model.fit(X_tr_res, y_tr_res)\n",
        "        probs = model.predict_proba(X_test)[:, 1]\n",
        "        print('Calculating metrics...')\n",
        "        # Calculate metrics\n",
        "        print('Calculating auc...')\n",
        "        auc_score = roc_auc_score(y_test, probs)\n",
        "        print('Calculating fpr tpr...')\n",
        "        fpr, tpr, thresholds = roc_curve(y_test, probs)\n",
        "        ks_stat = max(abs(fpr - tpr))\n",
        "        print('Calculating hscore...')\n",
        "        h_measure_score = h_score(y_test, probs) # Assuming h_score is given\n",
        "        print('Metrics calculating done...')\n",
        "\n",
        "        # Append scores\n",
        "        auc_scores.append(auc_score)\n",
        "        ks_stats.append(ks_stat)\n",
        "        h_measures.append(h_measure_score)\n",
        "        print('Appending result done...')\n",
        "        # Find best cutoff, generate confusion matrix, and calculate losses\n",
        "        best_cutoff = thresholds[np.argmax(tpr - fpr)]\n",
        "        y_pred = (probs >= best_cutoff).astype(int)\n",
        "        cm = confusion_matrix(y_test, y_pred)\n",
        "        cm_sum += cm\n",
        "        print('Confusion matrix done...')\n",
        "        actual_loss = np.sum(X_test[:, loss_col_name] * y_test)\n",
        "        predicted_loss = np.sum(X_test[:, loss_col_name] * y_pred)\n",
        "        predicted_loss_in = np.sum(X_test[:, loss_col_name] * y_pred * y_test)\n",
        "        actual_losses.append(actual_loss)\n",
        "        predicted_losses.append(predicted_loss)\n",
        "        predicted_losses_in.append(predicted_loss_in)\n",
        "\n",
        "        print('Plotting...')\n",
        "        # Plot Predictive vs Real (for each fold)\n",
        "        plot_calibration_curve(y_test, probs, ax, label=f'Fold {fold}')\n",
        "        fold += 1\n",
        "    plt.savefig(fig_name+'.png', dpi=300)\n",
        "    # Compute averages of scores and losses\n",
        "    avg_auc = np.mean(auc_scores)\n",
        "    avg_ks = np.mean(ks_stats)\n",
        "    avg_h_measure = np.mean(h_measures)\n",
        "    avg_actual_loss = np.mean(actual_losses)\n",
        "    avg_predicted_loss = np.mean(predicted_losses)\n",
        "    avg_predicted_loss_in = np.mean(predicted_losses_in)\n",
        "    loss_difference = avg_actual_loss - avg_predicted_loss\n",
        "    loss_difference_in = avg_actual_loss - avg_predicted_loss_in\n",
        "\n",
        "    # Print average metrics and loss information\n",
        "    print(f\"Average AUC Score: {avg_auc}\")\n",
        "    print(f\"Average KS Statistic: {avg_ks}\")\n",
        "    print(f\"Average H-Measure: {avg_h_measure}\")\n",
        "    print(f\"Cumulative Confusion Matrix:\\n{cm_sum}\")\n",
        "    print(f\"Average Actual Loss: {avg_actual_loss}\")\n",
        "    print(f\"Average Predicted Loss: {avg_predicted_loss}\")\n",
        "    print(f\"Average Predicted Loss: {avg_predicted_loss_in}\")\n",
        "    print(f\"Loss Difference: {loss_difference}\")\n",
        "    print(f\"Loss Difference: {loss_difference_in}\")\n",
        "\n",
        "    # Return metrics and losses for further use if needed\n",
        "    return avg_auc, avg_ks, avg_h_measure, cm_sum, avg_actual_loss, avg_predicted_loss, avg_predicted_loss_in, loss_difference, loss_difference_in\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-khCMnF6F8e"
      },
      "outputs": [],
      "source": [
        "X_train = pd.read_csv(path + \"GvB_train_X.csv\")\n",
        "X_test = pd.read_csv(path + \"GvB_test_X.csv\")\n",
        "y_train = pd.read_csv(path + \"GvB_train_y.csv\", header = None)\n",
        "y_test = pd.read_csv(path + \"GvB_test_y.csv\",header = None)\n",
        "\n",
        "variable_list = [\"annuity2principal\",\"dti_x\",\"tot_hi_cred_lim\",\"annual_inc\",\"total_bc_limit\",\n",
        "                 \"revol2inc\",\"total_il_high_credit_limit\",\"total_bal_ex_mort\",\"revol_util\",\n",
        "                 \"CreditHistoryLength\",\"MONTHLYCONTRACTAMT\",\"income2principal\",\n",
        "                 \"revol_bal\",\"total_rev_hi_lim\",\"PBAL_BEG_PERIOD\"]\n",
        "\n",
        "X_train = X_train[variable_list].values\n",
        "X_test = X_test[variable_list].values\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqIE0qCWxR0f"
      },
      "outputs": [],
      "source": [
        "X = np.vstack([X_train, X_test])\n",
        "y = np.vstack([y_train, y_test]).ravel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "559GJlCmspOv"
      },
      "outputs": [],
      "source": [
        "X[:,-1] * y.ravel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-glSC7m9CDA"
      },
      "source": [
        "### LC Dataset - Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BBC4A63KwLOv"
      },
      "outputs": [],
      "source": [
        "lgbm = lgb.LGBMClassifier(boosting_type ='gbdt',\n",
        "                      class_weight='balanced',\n",
        "                      learning_rate= 0.01906318880018833,\n",
        "                      max_depth = 7,\n",
        "                      num_leaves = 44,\n",
        "                      objective='binary',\n",
        "                      n_jobs = -1,\n",
        "                      metric = 'auc',\n",
        "                      verbose=-1)\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=500, n_jobs=-1, class_weight=\"balanced\", max_depth=7, max_leaf_nodes=44,\n",
        "                            min_samples_leaf=480)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPbJ6yYgv-7H"
      },
      "outputs": [],
      "source": [
        "evaluate_model(lgbm, X, y, -1, 'lgbm_cv5_calibration')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNwcFyf1N_IA"
      },
      "outputs": [],
      "source": [
        "evaluate_model(rf, X, y, -1,'rf_cv5_calibration')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCjhytL8SE3V"
      },
      "outputs": [],
      "source": [
        "lr = LogisticRegression(solver=\"sag\", max_iter=10000, class_weight=\"balanced\")\n",
        "evaluate_model(lr, X, y, -1, 'lr_cv5_calibration')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seHOTcCfcU7l"
      },
      "outputs": [],
      "source": [
        "tau = -0.3\n",
        "params = {\"objective\":\"binary\", \"boosting_type\":\"gbdt\", \"verbose\": -1, \"metric\": \"auc\", 'learning_rate': 0.01906318880018833, 'max_depth': -1,  \"objective\": custom_loss}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2syKUrWkatk3"
      },
      "outputs": [],
      "source": [
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "auc_scores = []\n",
        "ks_stats = []\n",
        "h_measures = []\n",
        "cm_sum = np.array([[0, 0], [0, 0]]) # Initialize a confusion matrix sum\n",
        "actual_losses = []\n",
        "predicted_losses = []\n",
        "predicted_losses_in = []\n",
        "fold = 1\n",
        "\n",
        "for train_index, test_index in tscv.split(X):\n",
        "    print('Fold trial starts...')\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    lgb_train = lgb.Dataset(X_train, y_train)\n",
        "    lgb_test = lgb.Dataset(X_test, y_test, reference = lgb_train)\n",
        "\n",
        "    gbm = lgb.train(params,\n",
        "                lgb_train,\n",
        "                valid_sets = lgb_test,\n",
        "                num_boost_round = 1000,\n",
        "                callbacks=[lgb.early_stopping(stopping_rounds=50),])\n",
        "\n",
        "    probs = gbm.predict(X_test)\n",
        "    probs = transform_gev(probs, -0.3)\n",
        "    print('Calculating metrics...')\n",
        "    # Calculate metrics\n",
        "    print('Calculating auc...')\n",
        "    auc_score = roc_auc_score(y_test, probs)\n",
        "    print('Calculating fpr tpr...')\n",
        "    fpr, tpr, thresholds = roc_curve(y_test, probs)\n",
        "    ks_stat = max(abs(fpr - tpr))\n",
        "    print('Calculating hscore...')\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "\n",
        "    # Fit and transform the data\n",
        "    probs_std = np.clip(probs_std, 0,1)\n",
        "    h_measure_score = h_score(y_test, probs_std) # Assuming h_score is given\n",
        "    print('Metrics calculating done...')\n",
        "\n",
        "    # Append scores\n",
        "    auc_scores.append(auc_score)\n",
        "    ks_stats.append(ks_stat)\n",
        "    h_measures.append(h_measure_score)\n",
        "    print('Appending result done...')\n",
        "    # Find best cutoff, generate confusion matrix, and calculate losses\n",
        "    best_cutoff = thresholds[np.argmax(tpr - fpr)]\n",
        "    y_pred = (probs >= best_cutoff).astype(int)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    cm_sum += cm\n",
        "    print('Confusion matrix done...')\n",
        "    actual_loss = np.sum(X_test[:, -1] * y_test)\n",
        "    predicted_loss = np.sum(X_test[:, -1] * y_pred)\n",
        "    predicted_loss_in = np.sum(X_test[:,-1] * y_pred * y_test)\n",
        "    actual_losses.append(actual_loss)\n",
        "    predicted_losses.append(predicted_loss)\n",
        "    predicted_losses_in.append(predicted_loss_in)\n",
        "    print('Plotting...')\n",
        "    # Plot Predictive vs Real (for each fold)\n",
        "    plot_calibration_curve(y_test, probs_std, ax, label=f'Fold {fold}')\n",
        "    fold += 1\n",
        "\n",
        "plt.savefig('gev_gbdt_scaler.png', dpi=300)\n",
        "# Compute averages of scores and losses\n",
        "avg_auc = np.mean(auc_scores)\n",
        "avg_ks = np.mean(ks_stats)\n",
        "avg_h_measure = np.mean(h_measures)\n",
        "avg_actual_loss = np.mean(actual_losses)\n",
        "avg_predicted_loss = np.mean(predicted_losses)\n",
        "avg_predicted_loss_in = np.mean(predicted_losses_in)\n",
        "loss_difference = avg_actual_loss - avg_predicted_loss\n",
        "loss_difference_in = avg_actual_loss - avg_predicted_loss_in\n",
        "# Print average metrics and loss information\n",
        "print(f\"Average AUC Score: {avg_auc}\")\n",
        "print(f\"Average KS Statistic: {avg_ks}\")\n",
        "print(f\"Average H-Measure: {avg_h_measure}\")\n",
        "print(f\"Cumulative Confusion Matrix:\\n{cm_sum}\")\n",
        "print(f\"Average Actual Loss: {avg_actual_loss}\")\n",
        "print(f\"Average Predicted Loss: {avg_predicted_loss}\")\n",
        "print(f\"Average Predicted Loss: {avg_predicted_loss_in}\")\n",
        "print(f\"Loss Difference: {loss_difference}\")\n",
        "print(f\"Loss Difference: {loss_difference_in}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xw0Jzj2c7rPF"
      },
      "outputs": [],
      "source": [
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "fig, ax = plt.subplots(figsize=(12,8))\n",
        "auc_scores = []\n",
        "ks_stats = []\n",
        "h_measures = []\n",
        "cm_sum = np.array([[0, 0], [0, 0]]) # Initialize a confusion matrix sum\n",
        "actual_losses = []\n",
        "predicted_losses = []\n",
        "predicted_losses_in = []\n",
        "fold = 1\n",
        "\n",
        "for train_index, test_index in tscv.split(X):\n",
        "    print('Fold trial starts...')\n",
        "\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    lgb_train = lgb.Dataset(X_train, y_train)\n",
        "    lgb_test = lgb.Dataset(X_test, y_test, reference = lgb_train)\n",
        "\n",
        "    gbm = lgb.train(params,\n",
        "                lgb_train,\n",
        "                valid_sets = lgb_test,\n",
        "                num_boost_round = 1000,\n",
        "                callbacks=[lgb.early_stopping(stopping_rounds=50),])\n",
        "\n",
        "    probs = gbm.predict(X_test)\n",
        "    probs = transform_gev(probs, -0.3)\n",
        "    print('Calculating metrics...')\n",
        "    # Calculate metrics\n",
        "    print('Calculating auc...')\n",
        "    auc_score = roc_auc_score(y_test, probs)\n",
        "    print('Calculating fpr tpr...')\n",
        "    fpr, tpr, thresholds = roc_curve(y_test, probs)\n",
        "    ks_stat = max(abs(fpr - tpr))\n",
        "    print('Calculating hscore...')\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    # Fit and transform the data\n",
        "    probs_std = scaler.fit_transform(probs.reshape(-1,1))\n",
        "    probs_std = np.clip(probs_std, 0,1)\n",
        "    h_measure_score = h_score(y_test, probs) # Assuming h_score is given\n",
        "    print('Metrics calculating done...')\n",
        "\n",
        "    # Append scores\n",
        "    auc_scores.append(auc_score)\n",
        "    ks_stats.append(ks_stat)\n",
        "    h_measures.append(h_measure_score)\n",
        "    print('Appending result done...')\n",
        "    # Find best cutoff, generate confusion matrix, and calculate losses\n",
        "    best_cutoff = thresholds[np.argmax(tpr - fpr)]\n",
        "    y_pred = (probs >= best_cutoff).astype(int)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    cm_sum += cm\n",
        "    print('Confusion matrix done...')\n",
        "    actual_loss = np.sum(X_test[:, -1] * y_test)\n",
        "    predicted_loss = np.sum(X_test[:, -1] * y_pred)\n",
        "    predicted_loss_in = np.sum(X_test[:,-1] * y_pred * y_test)\n",
        "    actual_losses.append(actual_loss)\n",
        "    predicted_losses.append(predicted_loss)\n",
        "    predicted_losses_in.append(predicted_loss_in)\n",
        "    print('Plotting...')\n",
        "    # Plot Predictive vs Real (for each fold)\n",
        "    plot_calibration_curve(y_test, probs, ax, label=f'Fold {fold}')\n",
        "    fold += 1\n",
        "\n",
        "plt.savefig('gev_gbdt_link.png', dpi=300)\n",
        "# Compute averages of scores and losses\n",
        "avg_auc = np.mean(auc_scores)\n",
        "avg_ks = np.mean(ks_stats)\n",
        "avg_h_measure = np.mean(h_measures)\n",
        "avg_actual_loss = np.mean(actual_losses)\n",
        "avg_predicted_loss = np.mean(predicted_losses)\n",
        "avg_predicted_loss_in = np.mean(predicted_losses_in)\n",
        "loss_difference = avg_actual_loss - avg_predicted_loss\n",
        "loss_difference_in = avg_actual_loss - avg_predicted_loss_in\n",
        "# Print average metrics and loss information\n",
        "print(f\"Average AUC Score: {avg_auc}\")\n",
        "print(f\"Average KS Statistic: {avg_ks}\")\n",
        "print(f\"Average H-Measure: {avg_h_measure}\")\n",
        "print(f\"Cumulative Confusion Matrix:\\n{cm_sum}\")\n",
        "print(f\"Average Actual Loss: {avg_actual_loss}\")\n",
        "print(f\"Average Predicted Loss: {avg_predicted_loss}\")\n",
        "print(f\"Average Predicted Loss in: {avg_predicted_loss_in}\")\n",
        "print(f\"Loss Difference: {loss_difference}\")\n",
        "print(f\"Loss Difference in: {loss_difference_in}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QPZTpU61tjSC"
      },
      "outputs": [],
      "source": [
        "lr = LogisticRegression(solver=\"sag\", max_iter=10000)\n",
        "evaluate_model_smote(lr, X, y, -1, 'lr_smote_cv5_calibration')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YrMltLuMtZ7j"
      },
      "outputs": [],
      "source": [
        "lgbm = lgb.LGBMClassifier(boosting_type ='gbdt',\n",
        "                      learning_rate= 0.01906318880018833,\n",
        "                      max_depth = 7,\n",
        "                      num_leaves = 44,\n",
        "                      objective='binary',\n",
        "                      n_jobs = -1,\n",
        "                      metric = 'auc',\n",
        "                      verbose=-1)\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=500, n_jobs=-1, max_depth=7, max_leaf_nodes=44,\n",
        "                            min_samples_leaf=480)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GakoEqj4tjjv"
      },
      "outputs": [],
      "source": [
        "evaluate_model_smote(rf, X, y, -1, 'rf_smote_cv5_calibration')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rv_c5z3mtjle"
      },
      "outputs": [],
      "source": [
        "evaluate_model_smote(lgbm, X, y, -1, 'lgbm_smote_cv5_calibration')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4yWaFwD4NYe"
      },
      "source": [
        "## SMF-1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYYY5zJC4PnA"
      },
      "outputs": [],
      "source": [
        "df_sbf1 = pd.read_csv(path + \"sbf_v2.csv\")\n",
        "df_sbf1 = df_sbf1.iloc[:,1:]\n",
        "Categorical_list = ['The status of bank accounts','Sales range of products', \"Educational Background\",\n",
        "                   \"Location\",\"Industry\",\"Loan type\",\"Currency Type\"]\n",
        "for i in Categorical_list:\n",
        "    df_sbf1 = df_sbf1.join(pd.get_dummies(df_sbf1[i], dtype = np.float64))\n",
        "df_sbf1 = df_sbf1.drop(columns = Categorical_list)\n",
        "print(df_sbf1.shape)\n",
        "print(df_sbf1.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0aW3ZJr6qBi"
      },
      "outputs": [],
      "source": [
        "Independent = ['Total Asset',\n",
        " 'Total Liability',\n",
        " 'TL/TA',\n",
        " 'OCF/CL',\n",
        " 'Quick Ratio',\n",
        " 'Current Ratio',\n",
        " 'Cash/Revenue from Operation',\n",
        " 'EBIT/CL',\n",
        " 'NCL/(NCL+E)',\n",
        " 'OCF/TA',\n",
        " 'Equity Ratio',\n",
        " 'Acid-test Ratio',\n",
        " 'OCF/Net Profit',\n",
        " 'E/(Clending + NCLending)',\n",
        " 'NFA/E',\n",
        " 'Cash Ratio',\n",
        " '(E+NCL)/(FA+Investment)',\n",
        " 'Total Unpaid Lending/Equity',\n",
        " 'Total Unpaid Lending/TA',\n",
        " 'OCF/TL',\n",
        " 'EBITDA/TL',\n",
        " 'ROE',\n",
        " 'OCF/Revenue',\n",
        " 'Net Profit Margin on Sales',\n",
        " 'ROA',\n",
        " 'Operating Profit Ratio',\n",
        " 'Net Profit/Total Expense and Cost',\n",
        " 'Gross Profit Margin',\n",
        " 'Net Profit/ Cost and Expense',\n",
        " 'EBITDA',\n",
        " 'EBITDA/Revenue',\n",
        " 'Net Profit',\n",
        " 'OCF',\n",
        " 'CF from Operation Activities',\n",
        " 'Receivables Turnover Ratio',\n",
        " 'Inventory Turnover Ratio',\n",
        " 'Total Asset Turnover',\n",
        " 'Current Asset Turnover',\n",
        " 'Fixed Asset Turnover',\n",
        " 'Equity Turnover',\n",
        " 'Working Capital/CA',\n",
        " 'Rate of Return on Investment',\n",
        " 'Payable Turnover Ratio',\n",
        " 'Cash Conversion Cycle',\n",
        " 'Revenue Growth',\n",
        " 'Net Profit Growth',\n",
        " 'TA Growth',\n",
        " 'Rate of Capital Accumulation',\n",
        " 'R/E Growth',\n",
        " ' Years of employment in relevant industry',\n",
        " 'Audited or Not',\n",
        " 'Recognized level of new product',\n",
        " 'Patent Status',\n",
        " 'Date of Establishment',\n",
        " 'Level of famous products',\n",
        " 'The proportion of total amount of money collected by enterprises through this bank',\n",
        " 'lending default record of legal representative',\n",
        " 'Credit card record of legal representative',\n",
        " 'Marital Status',\n",
        " 'Residential Status',\n",
        " 'Year of residence of legal representative',\n",
        " 'Gender',\n",
        " 'Age',\n",
        " 'The value of vehicle and real estates of legal representative',\n",
        " 'Year of employment in this position',\n",
        " 'Category of registered capital ',\n",
        " 'Enterprise credit status in the past three years',\n",
        " 'Corporate tax record',\n",
        " 'Legal Issue',\n",
        " 'Compliance Status',\n",
        " 'Number of Defaults',\n",
        " 'Industry Index',\n",
        " 'Year-end balance of per capita savings of urban and rural residents (yuan/person)',\n",
        " 'GDP Growth Rate',\n",
        " 'CPI',\n",
        " 'Per capita disposable income of urban residents (yuan) / person)',\n",
        " 'Engel coefficient',\n",
        " 'Principal',      # 77\n",
        " 'Amount of Interests and Principals',\n",
        " 'Score of pledge/collateral',\n",
        " 'General Account',\n",
        " 'No account or Missing',\n",
        " 'Primary Account',\n",
        " 'Domestically',\n",
        " 'Internationally',\n",
        " 'Others or Missing',\n",
        " '3yr College',\n",
        " 'Bachelor+',\n",
        " 'High School',\n",
        " 'Middle/Primary School',\n",
        " 'Others or missing',\n",
        " '上海市',\n",
        " '北京市',\n",
        " '四川成都市',\n",
        " '四川省乐山市',\n",
        " '四川省凉山州',\n",
        " '四川省南充市',\n",
        " '四川省广安市',\n",
        " '四川省彭州市',\n",
        " '四川省攀枝花市',\n",
        " '四川省自贡市',\n",
        " '四川省资阳市',\n",
        " '四川省邛崃市',\n",
        " '四川雅安',\n",
        " '天津市',\n",
        " '江西吉安',\n",
        " '河北省唐山市',\n",
        " '河南省新乡市',\n",
        " '辽宁省丹东市',\n",
        " '辽宁省大连市',\n",
        " '辽宁省本溪市',\n",
        " '辽宁省沈阳市',\n",
        " '辽宁省盘锦市',\n",
        " '辽宁省营口市',\n",
        " '辽宁省铁岭市',\n",
        " '辽宁省锦州市',\n",
        " '辽宁省鞍山市',\n",
        " '重庆市',\n",
        " '交通运输业企业',\n",
        " '仓储企业',\n",
        " '住宿、餐饮业',\n",
        " '信息传输业',\n",
        " '其他企业',\n",
        " '工业企业',\n",
        " '建筑企业',\n",
        " '房地产开发经营',\n",
        " '批发业企业',\n",
        " '租赁和商务服务业',\n",
        " '软件和信息技术服务业',\n",
        " '零售业企业',\n",
        " '一般中长期贷款',\n",
        " '中长期房地产开发贷款',\n",
        " '中长期项目贷款',\n",
        " '信用证项下打包贷款',\n",
        " '信用证项下进口押汇',\n",
        " '出口信用证押汇',\n",
        " '出口商业发票融资',\n",
        " '商业承兑汇票贴现',\n",
        " '应收账款质押贷款',\n",
        " '短期流动资金贷款',\n",
        " '短期项目贷款',\n",
        " '进口代收押汇',\n",
        " '银行承兑汇票贴现',\n",
        " '人民币',\n",
        " '日元',\n",
        " '欧元',\n",
        " '港币',\n",
        " '美元']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLCZwgcG66W6"
      },
      "outputs": [],
      "source": [
        "df_sbf1.isnull().any().any()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybKji5ww8nZJ"
      },
      "outputs": [],
      "source": [
        "X = df_sbf1[Independent].values\n",
        "y = df_sbf1[\"Default\"].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZPQax38JkaM"
      },
      "outputs": [],
      "source": [
        "lgbm = lgb.LGBMClassifier(boosting_type ='gbdt',\n",
        "                      class_weight='balanced',\n",
        "                      learning_rate= 0.01906318880018833,\n",
        "                      max_depth = 7,\n",
        "                      num_leaves = 44,\n",
        "                      objective='binary',\n",
        "                      n_jobs = -1,\n",
        "                      metric = 'auc',\n",
        "                      verbose=-1)\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=500, n_jobs=-1, class_weight=\"balanced\", max_depth=7, max_leaf_nodes=44,\n",
        "                            min_samples_leaf=480)\n",
        "\n",
        "#evaluate_model(lgbm, X, y, 77, 'lgbm_smf1_calibration')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2g3Hb-x-Jstv"
      },
      "outputs": [],
      "source": [
        "evaluate_model(lgbm, X, y, 77, 'lgbm_smf1_calibration')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1Wsw_MVMBKI"
      },
      "outputs": [],
      "source": [
        "evaluate_model(rf, X, y, 77, 'rf_smf1_calibration')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "voFKhtJGMYOr"
      },
      "outputs": [],
      "source": [
        "lr = LogisticRegression(solver=\"sag\", max_iter=10000, class_weight=\"balanced\")\n",
        "evaluate_model(lr, X, y, 77, 'lr_smf1_calibration')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SrS33D8QRuOr"
      },
      "outputs": [],
      "source": [
        "lr = LogisticRegression(solver=\"sag\", max_iter=10000)\n",
        "evaluate_model_smote(lr, X, y, 77, 'lr_smf1_calibration')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agmaxr3IkMmB"
      },
      "outputs": [],
      "source": [
        "lgbm = lgb.LGBMClassifier(boosting_type ='gbdt',\n",
        "                      learning_rate= 0.01906318880018833,\n",
        "                      max_depth = 7,\n",
        "                      num_leaves = 44,\n",
        "                      objective='binary',\n",
        "                      n_jobs = -1,\n",
        "                      metric = 'auc',\n",
        "                      verbose=-1)\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=500, n_jobs=-1, max_depth=7, max_leaf_nodes=44,\n",
        "                            min_samples_leaf=480)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vuM6PZzxYAYn"
      },
      "outputs": [],
      "source": [
        "evaluate_model_smote(rf, X, y, 77, 'rf_smf1_calibration_smote')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H52qXbwbYAfc"
      },
      "outputs": [],
      "source": [
        "evaluate_model_smote(lgbm, X, y, 77, 'lr_smf1_calibration_smote')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdTIGeuTqB4D"
      },
      "outputs": [],
      "source": [
        "tau = -0.45\n",
        "params = {\"objective\":\"binary\", \"boosting_type\":\"gbdt\", \"verbose\": -1, \"metric\": \"auc\", 'learning_rate': 0.01906318880018833, 'max_depth': -1,  \"objective\": custom_loss}\n",
        "\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "fig, ax = plt.subplots(figsize=(12,8))\n",
        "auc_scores = []\n",
        "ks_stats = []\n",
        "h_measures = []\n",
        "cm_sum = np.array([[0, 0], [0, 0]]) # Initialize a confusion matrix sum\n",
        "actual_losses = []\n",
        "predicted_losses = []\n",
        "predicted_losses_in = []\n",
        "fold = 1\n",
        "\n",
        "for train_index, test_index in tscv.split(X):\n",
        "    print('Fold trial starts...')\n",
        "\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    lgb_train = lgb.Dataset(X_train, y_train)\n",
        "    lgb_test = lgb.Dataset(X_test, y_test, reference = lgb_train)\n",
        "\n",
        "    gbm = lgb.train(params,\n",
        "                lgb_train,\n",
        "                valid_sets = lgb_test,\n",
        "                num_boost_round = 1000,\n",
        "                callbacks=[lgb.early_stopping(stopping_rounds=50),])\n",
        "\n",
        "    probs = gbm.predict(X_test)\n",
        "    probs = transform_gev(probs, -0.45)\n",
        "    print('Calculating metrics...')\n",
        "    # Calculate metrics\n",
        "    print('Calculating auc...')\n",
        "    auc_score = roc_auc_score(y_test, probs)\n",
        "    print('Calculating fpr tpr...')\n",
        "    fpr, tpr, thresholds = roc_curve(y_test, probs)\n",
        "    ks_stat = max(abs(fpr - tpr))\n",
        "    print('Calculating hscore...')\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    # Fit and transform the data\n",
        "    probs_std = scaler.fit_transform(probs.reshape(-1,1))\n",
        "    probs_std = np.clip(probs_std, 0,1)\n",
        "    h_measure_score = h_score(y_test, probs) # Assuming h_score is given\n",
        "    print('Metrics calculating done...')\n",
        "\n",
        "    # Append scores\n",
        "    auc_scores.append(auc_score)\n",
        "    ks_stats.append(ks_stat)\n",
        "    h_measures.append(h_measure_score)\n",
        "    print('Appending result done...')\n",
        "    # Find best cutoff, generate confusion matrix, and calculate losses\n",
        "    best_cutoff = thresholds[np.argmax(tpr - fpr)]\n",
        "    y_pred = (probs >= best_cutoff).astype(int)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    cm_sum += cm\n",
        "    print('Confusion matrix done...')\n",
        "    actual_loss = np.sum(X_test[:, 77] * y_test)\n",
        "    predicted_loss = np.sum(X_test[:, 77] * y_pred)\n",
        "    predicted_loss_in = np.sum(X_test[:,77] * y_pred * y_test)\n",
        "    actual_losses.append(actual_loss)\n",
        "    predicted_losses.append(predicted_loss)\n",
        "    predicted_losses_in.append(predicted_loss_in)\n",
        "    print('Plotting...')\n",
        "    # Plot Predictive vs Real (for each fold)\n",
        "    plot_calibration_curve(y_test, probs, ax, label=f'Fold {fold}')\n",
        "    fold += 1\n",
        "\n",
        "plt.savefig('gev_gbdt_link.png', dpi=300)\n",
        "# Compute averages of scores and losses\n",
        "avg_auc = np.mean(auc_scores)\n",
        "avg_ks = np.mean(ks_stats)\n",
        "avg_h_measure = np.mean(h_measures)\n",
        "avg_actual_loss = np.mean(actual_losses)\n",
        "avg_predicted_loss = np.mean(predicted_losses)\n",
        "avg_predicted_loss_in = np.mean(predicted_losses_in)\n",
        "loss_difference = avg_actual_loss - avg_predicted_loss\n",
        "loss_difference_in = avg_actual_loss - avg_predicted_loss_in\n",
        "# Print average metrics and loss information\n",
        "print(f\"Average AUC Score: {avg_auc}\")\n",
        "print(f\"Average KS Statistic: {avg_ks}\")\n",
        "print(f\"Average H-Measure: {avg_h_measure}\")\n",
        "print(f\"Cumulative Confusion Matrix:\\n{cm_sum}\")\n",
        "print(f\"Average Actual Loss: {avg_actual_loss}\")\n",
        "print(f\"Average Predicted Loss: {avg_predicted_loss}\")\n",
        "print(f\"Average Predicted Loss in: {avg_predicted_loss_in}\")\n",
        "print(f\"Loss Difference: {loss_difference}\")\n",
        "print(f\"Loss Difference in: {loss_difference_in}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SMF-2"
      ],
      "metadata": {
        "id": "Qd4FM3k7GlpD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold"
      ],
      "metadata": {
        "id": "Nd-87a52cCZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model_kf(model, X, y, loss_col_name, fig_name):\n",
        "    tscv = KFold(n_splits=5)\n",
        "    fig, ax = plt.subplots(figsize=(12,8))\n",
        "    auc_scores = []\n",
        "    ks_stats = []\n",
        "    h_measures = []\n",
        "    cm_sum = np.array([[0, 0], [0, 0]]) # Initialize a confusion matrix sum\n",
        "    actual_losses = []\n",
        "    predicted_losses = []\n",
        "    predicted_losses_in = []\n",
        "    fold = 1\n",
        "    for train_index, test_index in tscv.split(X):\n",
        "        print('Fold trial starts...')\n",
        "        X_train, X_test = X[train_index], X[test_index]\n",
        "        y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "        model.fit(X_train, y_train)\n",
        "        probs = model.predict_proba(X_test)[:, 1]\n",
        "        print('Calculating metrics...')\n",
        "        # Calculate metrics\n",
        "        print('Calculating auc...')\n",
        "        auc_score = roc_auc_score(y_test, probs)\n",
        "        print('Calculating fpr tpr...')\n",
        "        fpr, tpr, thresholds = roc_curve(y_test, probs)\n",
        "        ks_stat = max(abs(fpr - tpr))\n",
        "        print('Calculating hscore...')\n",
        "        h_measure_score = h_score(y_test, probs) # Assuming h_score is given\n",
        "        print('Metrics calculating done...')\n",
        "\n",
        "        # Append scores\n",
        "        auc_scores.append(auc_score)\n",
        "        ks_stats.append(ks_stat)\n",
        "        h_measures.append(h_measure_score)\n",
        "        print('Appending result done...')\n",
        "        # Find best cutoff, generate confusion matrix, and calculate losses\n",
        "        best_cutoff = thresholds[np.argmax(tpr - fpr)]\n",
        "        y_pred = (probs >= best_cutoff).astype(int)\n",
        "        cm = confusion_matrix(y_test, y_pred)\n",
        "        cm_sum += cm\n",
        "        print('Confusion matrix done...')\n",
        "        actual_loss = np.sum(X_test[:, loss_col_name] * y_test)\n",
        "        predicted_loss = np.sum(X_test[:, loss_col_name] * y_pred)\n",
        "        predicted_loss_in = np.sum(X_test[:, loss_col_name] * y_pred * y_test)\n",
        "        actual_losses.append(actual_loss)\n",
        "        predicted_losses.append(predicted_loss)\n",
        "        predicted_losses_in.append(predicted_loss_in)\n",
        "\n",
        "        print('Plotting...')\n",
        "        # Plot Predictive vs Real (for each fold)\n",
        "        plot_calibration_curve(y_test, probs, ax, label=f'Fold {fold}')\n",
        "        fold += 1\n",
        "    plt.savefig(fig_name + '.png', dpi=300)\n",
        "    # Compute averages of scores and losses\n",
        "    avg_auc = np.mean(auc_scores)\n",
        "    avg_ks = np.mean(ks_stats)\n",
        "    avg_h_measure = np.mean(h_measures)\n",
        "    avg_actual_loss = np.mean(actual_losses)\n",
        "    avg_predicted_loss = np.mean(predicted_losses)\n",
        "    avg_predicted_loss_in = np.mean(predicted_losses_in)\n",
        "    loss_difference = avg_actual_loss - avg_predicted_loss\n",
        "    loss_difference_in = avg_actual_loss - avg_predicted_loss_in\n",
        "\n",
        "    # Print average metrics and loss information\n",
        "    print(f\"Average AUC Score: {avg_auc}\")\n",
        "    print(f\"Average KS Statistic: {avg_ks}\")\n",
        "    print(f\"Average H-Measure: {avg_h_measure}\")\n",
        "    print(f\"Cumulative Confusion Matrix:\\n{cm_sum}\")\n",
        "    print(f\"Average Actual Loss: {avg_actual_loss}\")\n",
        "    print(f\"Average Predicted Loss: {avg_predicted_loss}\")\n",
        "    print(f\"Average Predicted Loss: {avg_predicted_loss_in}\")\n",
        "    print(f\"Loss Difference: {loss_difference}\")\n",
        "    print(f\"Loss Difference: {loss_difference_in}\")\n",
        "\n",
        "    # Return metrics and losses for further use if needed\n",
        "    return avg_auc, avg_ks, avg_h_measure, cm_sum, avg_actual_loss, avg_predicted_loss, avg_predicted_loss_in, loss_difference, loss_difference_in\n",
        "\n",
        "def evaluate_model_smote_kf(model, X, y, loss_col_name, fig_name):\n",
        "    tscv =KFold(n_splits=5)\n",
        "    fig, ax = plt.subplots(figsize=(12,8))\n",
        "    auc_scores = []\n",
        "    ks_stats = []\n",
        "    h_measures = []\n",
        "    cm_sum = np.array([[0, 0], [0, 0]]) # Initialize a confusion matrix sum\n",
        "    actual_losses = []\n",
        "    predicted_losses = []\n",
        "    predicted_losses_in = []\n",
        "    fold = 1\n",
        "    for train_index, test_index in tscv.split(X):\n",
        "        print('Fold trial starts...')\n",
        "\n",
        "        X_train, X_test = X[train_index], X[test_index]\n",
        "        y_train, y_test = y[train_index], y[test_index]\n",
        "        print(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train==1)))\n",
        "        print(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train==0)))\n",
        "        sm = SMOTE(random_state = 202402)\n",
        "        X_tr_res, y_tr_res = sm.fit_resample(X_train, y_train)\n",
        "\n",
        "        print('After OverSampling, the shape of train_X: {}'.format(X_tr_res.shape))\n",
        "        print('After OverSampling, the shape of train_y: {} \\n'.format(y_tr_res.shape))\n",
        "\n",
        "        print(\"After OverSampling, counts of label '1': {}\".format(sum(y_tr_res==1)))\n",
        "        print(\"After OverSampling, counts of label '0': {}\".format(sum(y_tr_res==0)))\n",
        "\n",
        "        model.fit(X_tr_res, y_tr_res)\n",
        "        probs = model.predict_proba(X_test)[:, 1]\n",
        "        print('Calculating metrics...')\n",
        "        # Calculate metrics\n",
        "        print('Calculating auc...')\n",
        "        auc_score = roc_auc_score(y_test, probs)\n",
        "        print('Calculating fpr tpr...')\n",
        "        fpr, tpr, thresholds = roc_curve(y_test, probs)\n",
        "        ks_stat = max(abs(fpr - tpr))\n",
        "        print('Calculating hscore...')\n",
        "        h_measure_score = h_score(y_test, probs) # Assuming h_score is given\n",
        "        print('Metrics calculating done...')\n",
        "\n",
        "        # Append scores\n",
        "        auc_scores.append(auc_score)\n",
        "        ks_stats.append(ks_stat)\n",
        "        h_measures.append(h_measure_score)\n",
        "        print('Appending result done...')\n",
        "        # Find best cutoff, generate confusion matrix, and calculate losses\n",
        "        best_cutoff = thresholds[np.argmax(tpr - fpr)]\n",
        "        y_pred = (probs >= best_cutoff).astype(int)\n",
        "        cm = confusion_matrix(y_test, y_pred)\n",
        "        cm_sum += cm\n",
        "        print('Confusion matrix done...')\n",
        "        actual_loss = np.sum(X_test[:, loss_col_name] * y_test)\n",
        "        predicted_loss = np.sum(X_test[:, loss_col_name] * y_pred)\n",
        "        predicted_loss_in = np.sum(X_test[:, loss_col_name] * y_pred * y_test)\n",
        "        actual_losses.append(actual_loss)\n",
        "        predicted_losses.append(predicted_loss)\n",
        "        predicted_losses_in.append(predicted_loss_in)\n",
        "\n",
        "        print('Plotting...')\n",
        "        # Plot Predictive vs Real (for each fold)\n",
        "        plot_calibration_curve(y_test, probs, ax, label=f'Fold {fold}')\n",
        "        fold += 1\n",
        "    plt.savefig(fig_name+'.png', dpi=300)\n",
        "    # Compute averages of scores and losses\n",
        "    avg_auc = np.mean(auc_scores)\n",
        "    avg_ks = np.mean(ks_stats)\n",
        "    avg_h_measure = np.mean(h_measures)\n",
        "    avg_actual_loss = np.mean(actual_losses)\n",
        "    avg_predicted_loss = np.mean(predicted_losses)\n",
        "    avg_predicted_loss_in = np.mean(predicted_losses_in)\n",
        "    loss_difference = avg_actual_loss - avg_predicted_loss\n",
        "    loss_difference_in = avg_actual_loss - avg_predicted_loss_in\n",
        "\n",
        "    # Print average metrics and loss information\n",
        "    print(f\"Average AUC Score: {avg_auc}\")\n",
        "    print(f\"Average KS Statistic: {avg_ks}\")\n",
        "    print(f\"Average H-Measure: {avg_h_measure}\")\n",
        "    print(f\"Cumulative Confusion Matrix:\\n{cm_sum}\")\n",
        "    print(f\"Average Actual Loss: {avg_actual_loss}\")\n",
        "    print(f\"Average Predicted Loss: {avg_predicted_loss}\")\n",
        "    print(f\"Average Predicted Loss: {avg_predicted_loss_in}\")\n",
        "    print(f\"Loss Difference: {loss_difference}\")\n",
        "    print(f\"Loss Difference: {loss_difference_in}\")\n",
        "\n",
        "    # Return metrics and losses for further use if needed\n",
        "    return avg_auc, avg_ks, avg_h_measure, cm_sum, avg_actual_loss, avg_predicted_loss, avg_predicted_loss_in, loss_difference, loss_difference_in"
      ],
      "metadata": {
        "id": "8fqzQP1cTUqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sbf2 = pd.read_excel(path + \"sbf_2.xlsx\", header = 0)\n",
        "df_sbf2 = df_sbf2[['Y', 'X1', 'x2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'X9', 'X10',\n",
        "       'X11', 'X12', 'X13', 'X14', 'X15', 'X16', 'X17', 'X18', 'X19', 'X20',\n",
        "       'X21', 'X22', 'X23', 'X24', 'X25', 'X26', 'X27', 'X28', 'X29', 'X30',\n",
        "       'X31', 'X32', 'X33', 'X34', 'X35', 'X36', 'X37', 'X38', 'X39', 'X40',\n",
        "       'X41', 'X42', 'X43', 'X44', 'X45', 'X46', 'X47', 'X48', 'X49', 'X50',\n",
        "       'X51', 'X52', 'X53', 'X54', 'X55', 'X56', 'X57', 'X58', 'X59', 'X60']]\n",
        "print(df_sbf2.shape)\n",
        "print(df_sbf2.columns)"
      ],
      "metadata": {
        "id": "49ZE0BHWGktK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df_sbf2.drop(columns=['Y',\"X25\", \"X23\", \"X22\", \"X21\", \"X29\", \"X27\"]).values\n",
        "y = df_sbf2['Y'].values"
      ],
      "metadata": {
        "id": "g46xammfJgqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lgbm = lgb.LGBMClassifier(boosting_type ='gbdt',\n",
        "                      class_weight='balanced',\n",
        "                      learning_rate= 0.01906318880018833,\n",
        "                      max_depth = 7,\n",
        "                      num_leaves = 44,\n",
        "                      objective='binary',\n",
        "                      n_jobs = -1,\n",
        "                      metric = 'auc',\n",
        "                      verbose=-1)\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=500, n_jobs=-1, class_weight=\"balanced\", max_depth=7, max_leaf_nodes=44,\n",
        "                            min_samples_leaf=480)\n",
        "\n",
        "lr = LogisticRegression(solver=\"sag\", max_iter=10000, class_weight=\"balanced\")"
      ],
      "metadata": {
        "id": "-XCZ0GGwL_Iq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model_kf(lgbm, X, y, -1, 'lgbm_smf2_calibration')"
      ],
      "metadata": {
        "id": "M67_oB9CMUQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model_kf(rf, X, y, -1, 'rf_smf2_calibration')"
      ],
      "metadata": {
        "id": "6lw56rMSMWw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model_kf(lr, X, y, -1, 'lr_smf2_calibration')"
      ],
      "metadata": {
        "id": "e6JEMmkoMXCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lgbm = lgb.LGBMClassifier(boosting_type ='gbdt',\n",
        "                      learning_rate= 0.01906318880018833,\n",
        "                      max_depth = 7,\n",
        "                      num_leaves = 44,\n",
        "                      objective='binary',\n",
        "                      n_jobs = -1,\n",
        "                      metric = 'auc',\n",
        "                      verbose=-1)\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=500, n_jobs=-1, max_depth=7, max_leaf_nodes=44,\n",
        "                            min_samples_leaf=480)\n",
        "\n",
        "lr = LogisticRegression(solver=\"sag\", max_iter=10000)"
      ],
      "metadata": {
        "id": "PdgrTSl7MdZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model_smote_kf(lgbm, X, y, -1, 'lgbm_smf2_calibration_smote')"
      ],
      "metadata": {
        "id": "Vnu_71szMiVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model_smote_kf(lr, X, y, -1, 'lr_smf2_calibration_smote')"
      ],
      "metadata": {
        "id": "QPLtl6cqMlS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model_smote_kf(rf, X, y, -1, 'rf_smf2_calibration_smote')"
      ],
      "metadata": {
        "id": "Nvh8VHefMlUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tau = -0.35\n",
        "params = {\"objective\":\"binary\", \"boosting_type\":\"gbdt\", \"verbose\": -1, \"metric\": \"auc\", 'learning_rate': 0.01906318880018833, 'max_depth': -1,  \"objective\": custom_loss}\n",
        "\n",
        "tscv = KFold(n_splits=5)\n",
        "fig, ax = plt.subplots(figsize=(12,8))\n",
        "auc_scores = []\n",
        "ks_stats = []\n",
        "h_measures = []\n",
        "cm_sum = np.array([[0, 0], [0, 0]]) # Initialize a confusion matrix sum\n",
        "actual_losses = []\n",
        "predicted_losses = []\n",
        "predicted_losses_in = []\n",
        "fold = 1\n",
        "\n",
        "for train_index, test_index in tscv.split(X):\n",
        "    print('Fold trial starts...')\n",
        "\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    lgb_train = lgb.Dataset(X_train, y_train)\n",
        "    lgb_test = lgb.Dataset(X_test, y_test, reference = lgb_train)\n",
        "\n",
        "    gbm = lgb.train(params,\n",
        "                lgb_train,\n",
        "                valid_sets = lgb_test,\n",
        "                num_boost_round = 1000,\n",
        "                callbacks=[lgb.early_stopping(stopping_rounds=50),])\n",
        "\n",
        "    probs = gbm.predict(X_test)\n",
        "    probs = transform_gev(probs, -0.35)\n",
        "    print('Calculating metrics...')\n",
        "    # Calculate metrics\n",
        "    print('Calculating auc...')\n",
        "    auc_score = roc_auc_score(y_test, probs)\n",
        "    print('Calculating fpr tpr...')\n",
        "    fpr, tpr, thresholds = roc_curve(y_test, probs)\n",
        "    ks_stat = max(abs(fpr - tpr))\n",
        "    print('Calculating hscore...')\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    # Fit and transform the data\n",
        "    probs_std = scaler.fit_transform(probs.reshape(-1,1))\n",
        "    probs_std = np.clip(probs_std, 0,1)\n",
        "    h_measure_score = h_score(y_test, probs) # Assuming h_score is given\n",
        "    print('Metrics calculating done...')\n",
        "\n",
        "    # Append scores\n",
        "    auc_scores.append(auc_score)\n",
        "    ks_stats.append(ks_stat)\n",
        "    h_measures.append(h_measure_score)\n",
        "    print('Appending result done...')\n",
        "    # Find best cutoff, generate confusion matrix, and calculate losses\n",
        "    best_cutoff = thresholds[np.argmax(tpr - fpr)]\n",
        "    y_pred = (probs >= best_cutoff).astype(int)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    cm_sum += cm\n",
        "    print('Confusion matrix done...')\n",
        "    actual_loss = np.sum(X_test[:, -1] * y_test)\n",
        "    predicted_loss = np.sum(X_test[:, -1] * y_pred)\n",
        "    predicted_loss_in = np.sum(X_test[:,-1] * y_pred * y_test)\n",
        "    actual_losses.append(actual_loss)\n",
        "    predicted_losses.append(predicted_loss)\n",
        "    predicted_losses_in.append(predicted_loss_in)\n",
        "    print('Plotting...')\n",
        "    # Plot Predictive vs Real (for each fold)\n",
        "    plot_calibration_curve(y_test, probs, ax, label=f'Fold {fold}')\n",
        "    fold += 1\n",
        "\n",
        "plt.savefig('gev_gbdt_link.png', dpi=300)\n",
        "# Compute averages of scores and losses\n",
        "avg_auc = np.mean(auc_scores)\n",
        "avg_ks = np.mean(ks_stats)\n",
        "avg_h_measure = np.mean(h_measures)\n",
        "avg_actual_loss = np.mean(actual_losses)\n",
        "avg_predicted_loss = np.mean(predicted_losses)\n",
        "avg_predicted_loss_in = np.mean(predicted_losses_in)\n",
        "loss_difference = avg_actual_loss - avg_predicted_loss\n",
        "loss_difference_in = avg_actual_loss - avg_predicted_loss_in\n",
        "# Print average metrics and loss information\n",
        "print(f\"Average AUC Score: {avg_auc}\")\n",
        "print(f\"Average KS Statistic: {avg_ks}\")\n",
        "print(f\"Average H-Measure: {avg_h_measure}\")\n",
        "print(f\"Cumulative Confusion Matrix:\\n{cm_sum}\")\n",
        "print(f\"Average Actual Loss: {avg_actual_loss}\")\n",
        "print(f\"Average Predicted Loss: {avg_predicted_loss}\")\n",
        "print(f\"Average Predicted Loss in: {avg_predicted_loss_in}\")\n",
        "print(f\"Loss Difference: {loss_difference}\")\n",
        "print(f\"Loss Difference in: {loss_difference_in}\")"
      ],
      "metadata": {
        "id": "ENuoU7OaeyrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SMF-3"
      ],
      "metadata": {
        "id": "FkFMi6fahnle"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(path + 'sbf_3_clean.csv')"
      ],
      "metadata": {
        "id": "uusyl87Fhmel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().any().any()"
      ],
      "metadata": {
        "id": "ZXuHQ0dSpwHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = df['是否违约'].values\n",
        "X = df.drop(columns=['是否违约']).values"
      ],
      "metadata": {
        "id": "jWsky7cFqcD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lgbm = lgb.LGBMClassifier(boosting_type ='gbdt',\n",
        "                      class_weight='balanced',\n",
        "                      learning_rate= 0.01906318880018833,\n",
        "                      max_depth = 7,\n",
        "                      num_leaves = 44,\n",
        "                      objective='binary',\n",
        "                      n_jobs = -1,\n",
        "                      metric = 'auc',\n",
        "                      verbose=-1)\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=500, n_jobs=-1, class_weight=\"balanced\", max_depth=7, max_leaf_nodes=44,\n",
        "                            min_samples_leaf=480)\n",
        "\n",
        "lr = LogisticRegression(solver=\"sag\", max_iter=10000, class_weight=\"balanced\")"
      ],
      "metadata": {
        "id": "5T-xTaswr-Yx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model_kf(lgbm, X, y, -1, 'lgbm_smf3_cs')"
      ],
      "metadata": {
        "id": "distWig9sFqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model_kf(lr, X, y, -1, 'lr_smf3_cs')"
      ],
      "metadata": {
        "id": "iMjQPW1ysNdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model_kf(rf, X, y, -1, 'rf_smf3_cs')"
      ],
      "metadata": {
        "id": "7LBZ9KDAsPe1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lgbm = lgb.LGBMClassifier(boosting_type ='gbdt',\n",
        "                      learning_rate= 0.01906318880018833,\n",
        "                      max_depth = 7,\n",
        "                      num_leaves = 44,\n",
        "                      objective='binary',\n",
        "                      n_jobs = -1,\n",
        "                      metric = 'auc',\n",
        "                      verbose=-1)\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=500, n_jobs=-1, max_depth=7, max_leaf_nodes=44,\n",
        "                            min_samples_leaf=480)\n",
        "\n",
        "lr = LogisticRegression(solver=\"sag\", max_iter=10000)"
      ],
      "metadata": {
        "id": "jNPmctZgsSkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model_kf(lgbm, X, y, -1, 'lgbm_smf3_smote')"
      ],
      "metadata": {
        "id": "PTsc_oMbsVEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model_kf(lr, X, y, -1, 'lr_smf3_smote')"
      ],
      "metadata": {
        "id": "_EkBtTRBsWgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model_kf(rf, X, y, -1, 'rf_smf3_smote')"
      ],
      "metadata": {
        "id": "8-T-V9LisWvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tau = -0.35\n",
        "params = {\"objective\":\"binary\", \"boosting_type\":\"gbdt\", \"verbose\": -1, \"metric\": \"auc\", 'learning_rate': 0.01906318880018833, 'max_depth': -1,  \"objective\": custom_loss}\n",
        "\n",
        "tscv = KFold(n_splits=5)\n",
        "fig, ax = plt.subplots(figsize=(12,8))\n",
        "auc_scores = []\n",
        "ks_stats = []\n",
        "h_measures = []\n",
        "cm_sum = np.array([[0, 0], [0, 0]]) # Initialize a confusion matrix sum\n",
        "actual_losses = []\n",
        "predicted_losses = []\n",
        "predicted_losses_in = []\n",
        "fold = 1\n",
        "\n",
        "for train_index, test_index in tscv.split(X):\n",
        "    print('Fold trial starts...')\n",
        "\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    lgb_train = lgb.Dataset(X_train, y_train)\n",
        "    lgb_test = lgb.Dataset(X_test, y_test, reference = lgb_train)\n",
        "\n",
        "    gbm = lgb.train(params,\n",
        "                lgb_train,\n",
        "                valid_sets = lgb_test,\n",
        "                num_boost_round = 1000,\n",
        "                callbacks=[lgb.early_stopping(stopping_rounds=50),])\n",
        "\n",
        "    probs = gbm.predict(X_test)\n",
        "    probs = transform_gev(probs, -0.35)\n",
        "    print('Calculating metrics...')\n",
        "    # Calculate metrics\n",
        "    print('Calculating auc...')\n",
        "    auc_score = roc_auc_score(y_test, probs)\n",
        "    print('Calculating fpr tpr...')\n",
        "    fpr, tpr, thresholds = roc_curve(y_test, probs)\n",
        "    ks_stat = max(abs(fpr - tpr))\n",
        "    print('Calculating hscore...')\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    # Fit and transform the data\n",
        "    probs_std = scaler.fit_transform(probs.reshape(-1,1))\n",
        "    probs_std = np.clip(probs_std, 0,1)\n",
        "    h_measure_score = h_score(y_test, probs) # Assuming h_score is given\n",
        "    print('Metrics calculating done...')\n",
        "\n",
        "    # Append scores\n",
        "    auc_scores.append(auc_score)\n",
        "    ks_stats.append(ks_stat)\n",
        "    h_measures.append(h_measure_score)\n",
        "    print('Appending result done...')\n",
        "    # Find best cutoff, generate confusion matrix, and calculate losses\n",
        "    best_cutoff = thresholds[np.argmax(tpr - fpr)]\n",
        "    y_pred = (probs >= best_cutoff).astype(int)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    cm_sum += cm\n",
        "    print('Confusion matrix done...')\n",
        "    actual_loss = np.sum(X_test[:, -1] * y_test)\n",
        "    predicted_loss = np.sum(X_test[:, -1] * y_pred)\n",
        "    predicted_loss_in = np.sum(X_test[:,-1] * y_pred * y_test)\n",
        "    actual_losses.append(actual_loss)\n",
        "    predicted_losses.append(predicted_loss)\n",
        "    predicted_losses_in.append(predicted_loss_in)\n",
        "    print('Plotting...')\n",
        "    # Plot Predictive vs Real (for each fold)\n",
        "    plot_calibration_curve(y_test, probs, ax, label=f'Fold {fold}')\n",
        "    fold += 1\n",
        "\n",
        "plt.savefig('gev_gbdt_link.png', dpi=300)\n",
        "# Compute averages of scores and losses\n",
        "avg_auc = np.mean(auc_scores)\n",
        "avg_ks = np.mean(ks_stats)\n",
        "avg_h_measure = np.mean(h_measures)\n",
        "avg_actual_loss = np.mean(actual_losses)\n",
        "avg_predicted_loss = np.mean(predicted_losses)\n",
        "avg_predicted_loss_in = np.mean(predicted_losses_in)\n",
        "loss_difference = avg_actual_loss - avg_predicted_loss\n",
        "loss_difference_in = avg_actual_loss - avg_predicted_loss_in\n",
        "# Print average metrics and loss information\n",
        "print(f\"Average AUC Score: {avg_auc}\")\n",
        "print(f\"Average KS Statistic: {avg_ks}\")\n",
        "print(f\"Average H-Measure: {avg_h_measure}\")\n",
        "print(f\"Cumulative Confusion Matrix:\\n{cm_sum}\")\n",
        "print(f\"Average Actual Loss: {avg_actual_loss}\")\n",
        "print(f\"Average Predicted Loss: {avg_predicted_loss}\")\n",
        "print(f\"Average Predicted Loss in: {avg_predicted_loss_in}\")\n",
        "print(f\"Loss Difference: {loss_difference}\")\n",
        "print(f\"Loss Difference in: {loss_difference_in}\")"
      ],
      "metadata": {
        "id": "jCJF_M38tPP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Economic Benefit Analysis"
      ],
      "metadata": {
        "id": "e2XS3XgId_lE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "models = ['CS-LR', 'CS-RF', 'CS-GBDT', 'SMOTE-LR', 'SMOTE-RF', 'SMOTE-GBDT', 'GEV-GBDT']\n",
        "actual_losses = np.array([898478589, 898478589, 898478589, 898478589, 898478589, 898478589, 898478589])\n",
        "predicted_losses = np.array([2191693360, 2141136208, 1977108477, 2237548541, 2041004443, 1978475722, 1924932136])\n",
        "error_magnitude = actual_losses - predicted_losses\n",
        "error_corrected = np.array([288488312, 239774684, 264696379, 278838311, 273127308, 253791874, 215355855])\n",
        "\n",
        "# Plot setup\n",
        "x = np.arange(len(models))  # the label locations\n",
        "width = 0.35  # the width of the bars\n",
        "# Plot setup for comparison\n",
        "fig, ax = plt.subplots(figsize=(14, 8))\n",
        "\n",
        "# Bar charts for errors\n",
        "rects1 = ax.bar(x - width/2, error_magnitude / 1e6, width, label='Actual Loss - Predicted Loss (Millions)', color='skyblue')\n",
        "rects2 = ax.bar(x + width/2, error_corrected / 1e6, width, label='Actual Loss - Predicted Loss Correct (Millions)', color='lightcoral')\n",
        "\n",
        "# Labels, title and custom x-axis tick labels, etc.\n",
        "ax.set_xlabel('Model')\n",
        "ax.set_ylabel('Error Magnitude (Millions)')\n",
        "ax.set_title('Error Magnitude Comparison by Model')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(models)\n",
        "ax.legend()\n",
        "\n",
        "# Annotate the smaller magnitude for GEV-GBDT\n",
        "def autolabel(rects):\n",
        "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
        "    for rect in rects:\n",
        "        height = rect.get_height()\n",
        "        ax.annotate('{}'.format(round(height, 2)),\n",
        "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
        "                    xytext=(0, 3),  # 3 points vertical offset\n",
        "                    textcoords=\"offset points\",\n",
        "                    ha='center', va='bottom')\n",
        "\n",
        "autolabel(rects1)\n",
        "autolabel(rects2)\n",
        "\n",
        "fig.tight_layout()\n",
        "\n",
        "plt.savefig('eco.png', dpi=300)"
      ],
      "metadata": {
        "id": "PmOzGgGtkT7e"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}